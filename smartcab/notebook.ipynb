{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h1>Project Report</h1>\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Implementation of a Basic Driving Agent</h2>\n",
    "\n",
    "As starting task, we will move the smartcab around the environment using \n",
    "a random approach. The set of possible actions will be: None, forward, \n",
    "left, right. The deadline will be set to false, but this doesn't mean\n",
    "that smartcat has an infinite number of moves as can see on code of the file\n",
    "\"environment.py\" (any way, enforcing it to false will increase a lot the\n",
    "number of moves available).\n",
    "\n",
    "The code corresponding to this agent can be found on the class \n",
    "\"RandomAgent\" at the file \"agents.py\".\n",
    "\n",
    "Observations from simulation:\n",
    "\n",
    "1. Normally the smartcab action is not optimal, but normally reaches the\n",
    "destination because has a lot of moves available to reach the destination.\n",
    "2. Sometimes the smartcab doesn't reachs the destination (very few cases).\n",
    "3. The environment  doesn't allow any agent to execute and action that\n",
    "violates traffic rules, but a strong negative reward is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulator.__init__(): Unable to import pygame; display disabled.\nImportError: No module named pygame\n"
     ]
    }
   ],
   "source": [
    "from smartcab.simulator import Simulator\n",
    "from smartcab.environment import Environment\n",
    "from smartcab.agents import RandomAgent\n",
    "\n",
    "environment = Environment()\n",
    "random_agent = RandomAgent(environment)\n",
    "environment.set_primary_agent(random_agent, enforce_deadline=False)\n",
    "\n",
    "simulator = Simulator(environment, update_delay=0.5, display=True)\n",
    "simulator.run(n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulator.__init__(): Unable to import pygame; display disabled.\nImportError: No module named pygame\n   iteration  q_size  cum_reward success\n0          1      23        -4.0   False\n"
     ]
    }
   ],
   "source": [
    "from smartcab.simulator import Simulator\n",
    "from smartcab.environment import Environment\n",
    "from smartcab.agents import QLearningAgent\n",
    "\n",
    "# Set up environment and agent\n",
    "e = Environment()  # create environment (also adds some dummy traffic)\n",
    "a = e.create_agent(QLearningAgent)  # create agent\n",
    "e.set_primary_agent(a, enforce_deadline=True)  # specify agent to track\n",
    "\n",
    "# Now simulate it\n",
    "sim = Simulator(e, update_delay=0.001, display=True)  # create simulator\n",
    "sim.run(n_trials=1)  # run for a specified number of trials\n",
    "\n",
    "a.stats_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Improve the Q-Learning Driving Agent</h2>\n",
    "\n",
    "Now let's tune the values for *learning rate (alpha)*, *the discount factor (gamma)* and the *exploration rate (epsilon)*. We will perform many simulations with many combinations of these parameters, and the we will report the results to see what is the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulator.run(): Trial 0\nEnvironment.reset(): Trial set up with start = (8, 1), destination = (3, 1), deadline = 25\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "<smartcab.agents.QLearningAgent object at 0x7fd78aefbe10>",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3d04c3ccc783>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_primary_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menforce_deadline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSimulator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/jcsastre/ml/udacity_machine-learning_smartcab/smartcab/simulator.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, n_trials)\u001b[0m\n\u001b[0;32m     90\u001b[0m                     \u001b[1;31m# Update environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_updated\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_delay\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_updated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jcsastre/ml/udacity_machine-learning_smartcab/smartcab/environment.pyc\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary_agent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m             \u001b[0magent_deadline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary_agent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'deadline'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0magent_deadline\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhard_time_limit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: <smartcab.agents.QLearningAgent object at 0x7fd78aefbe10>"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for alpha_rate in np.arange(0.00, 1.00, 0.05):\n",
    "    for epsilon_rate in np.arange(0.00, 1.00, 0.05):\n",
    "        for gamma_rate in np.arange(0.00, 1.00, 0.05):\n",
    "            e = Environment()\n",
    "            a = QLearningAgent(e, alpha_rate=alpha_rate, epsilon_rate=epsilon_rate, gamma_rate=gamma_rate)\n",
    "            e.set_primary_agent(a, enforce_deadline=True)\n",
    "            s = Simulator(e, update_delay=0.001, display=False)\n",
    "            s.run(n_trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}