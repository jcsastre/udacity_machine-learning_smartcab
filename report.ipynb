{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Self-Driving Agent Report</h1>\n",
    "\n",
    "<h2>1. Implementation of a Basic Driving Agent</h2>\n",
    "\n",
    "As starting task, we will move the smartcab around the environment using a random approach. The set of possible actions will be: None, forward, left, right. The deadline will be set to false, but this doesn't mean that smartcab has an infinite number of moves as can see on code of the file **smartcab/environment.py** (but will increase a lot the number of moves available).\n",
    "\n",
    "The code corresponding to this agent can be found on the class **RandomAgent** at the file **smartcab/agents.py**.\n",
    "\n",
    "Observations from simulation:\n",
    "\n",
    "1. Normally the smartcab action is not optimal, but normally reaches the destination because has a lot of moves available to reach the destination.\n",
    "2. The environment  doesn't allow any agent to execute and action that violates traffic rules, but a strong negative reward is applied.\n",
    "\n",
    "<h2>2. Inform the Driving Agent</h2>\n",
    "\n",
    "The next task  is to identify a set of states that are appropriate for modeling the smartcab and environment. \n",
    "\n",
    "All the information we receive come from the environment and the planner.\n",
    "\n",
    "Sensing the environment provide us with these inputs:\n",
    "\n",
    "- light:\n",
    "    - Possible values: Red / Green\n",
    "- oncoming:\n",
    "    - Possible values: None / Forward / Right / Left\n",
    "    - Indicates if there is a car oncoming and the action wants to execute.\n",
    "- right:\n",
    "    - Possible values: None / Forward / Right / Left\n",
    "    - Indicates if there is a car approaching from the right oncoming and \n",
    "    the action wants to execute.\n",
    "- left:\n",
    "    - Possible values: None / Forward / Right / Left\n",
    "    - Indicates if there is a car approaching from the left oncoming and \n",
    "    the action wants to execute.\n",
    "\n",
    "Also from the environment we can obtain the deadline, that is the number of remaining moves to reach the destination.\n",
    "\n",
    "The planner provides next_waypoint, with these possible values: Forward, Right and Left.\n",
    "\n",
    "For representing the state we will use: **next_waypoint**, **light**, **oncoming**, **right** and **left**.\n",
    "\n",
    "Having in mind we use *next_waypoint*, is not very useful to also use *deadline*. Also *deadline* will increase considerably the number of possible states, and would penalize the Q-Learning implementation.\n",
    "\n",
    "The information from *light*, *oncoming*, *right* and *left* can help Q-Learning to avoid traffic violations. The information from *next_waypoint* can help Q-Learning to reach the destination as soon as possible.\n",
    " \n",
    "Having in mind the properties used for the state, and possible values for each of these, the total number of different states are: 3 x 2 x 4 x 4 x 4. This means a total of 384 states at a given time.\n",
    "\n",
    "<h2>3. Implement a Q-Learning Driving Agent</h2>\n",
    "\n",
    "The third task is to implement the Q-Learning algorithm for the driving agent. The code corresponding to this agent can be found on the class **QLearningAgent** at the file **smartcab/agents.py**.\n",
    "\n",
    "The core of the algorithm is a simple value iteration update. It assumes the old value and makes a correction based on the new information (Source: [Wikipedia](https://en.wikipedia.org/wiki/Q-learning)):\n",
    "\n",
    "![](images/qlearn.png)\n",
    "\n",
    "Before proceeding to the simulation, some parameter values should be set.\n",
    "\n",
    "In the formula shown above, two contants can be seen:\n",
    "- **alpha_rate (α)** or **learning rate**: Determines to what extent the newly acquired information will override the old information.\n",
    "- **gamma rate (γ)** or **discount factor**: Determines the importance of future rewards.\n",
    "\n",
    "I will start with **alpha_rate = 0.9** and **gamma rate = 0.5**.\n",
    "\n",
    "Another important value is the **epsilon_rate (ε)** or **exploration rate**, that determines when to explore or when to exploit the already learn information. I will start with **epsilon_rate = 0.1**.\n",
    "\n",
    "Finally Q values should have an initial value. I will  use **0.0 as initial value**. Please notice that in the corresponding code I don't make a static initialization. Instead in the method **get_q_value** I return **self.q_init_value==0.0** that is equivalent:\n",
    "\n",
    "~~~~\n",
    "def get_q_value(self, state, action):\n",
    "    key = (state, action)\n",
    "    return self.q_matrix.get(key, self.q_init_value)\n",
    "~~~~\n",
    "\n",
    "The reason is that I've added code to build stats, and one of the values i want to track is the number of explored states. I get this number by simply using this code:\n",
    "\n",
    "~~~~\n",
    "len(self.q_matrix)\n",
    "~~~~\n",
    "\n",
    "The simulation will be executed 100 times, with enforce_deadline to True.\n",
    "\n",
    "The stats data has been stored in a file that I will analyze below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"smartcab/stats_first_qlearn_agent.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **data** dataframe is a table containing 100 rows (one by simulation iteration), and 5 columns:\n",
    "- **iteration**: The iteration number.\n",
    "- **success**: True if the agent reached the destination.\n",
    "- **cum_reward**: The accumulated reward in that iteration.\n",
    "- **explored_states_cum**: The accumulated number of states explored.\n",
    "- **traffic_violations_count**: The traffic violations that occurred in that iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Let's explore the 10 first iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration</th>\n",
       "      <th>success</th>\n",
       "      <th>cum_reward</th>\n",
       "      <th>explored_states_cum</th>\n",
       "      <th>traffic_violations_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>9.5</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>-7.5</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>4.0</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>7.0</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>12.0</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iteration success  cum_reward  explored_states_cum  \\\n",
       "0          1   False         5.0                   14   \n",
       "1          2   False         9.5                   22   \n",
       "2          3    True         0.0                   22   \n",
       "3          4   False         0.5                   24   \n",
       "4          5   False        -7.5                   27   \n",
       "5          6    True         4.0                   27   \n",
       "6          7   False         7.0                   27   \n",
       "7          8    True        -0.5                   27   \n",
       "8          9   False        12.0                   29   \n",
       "9         10   False        -7.0                   29   \n",
       "\n",
       "   traffic_violations_count  \n",
       "0                         7  \n",
       "1                         2  \n",
       "2                         0  \n",
       "3                         7  \n",
       "4                         7  \n",
       "5                         3  \n",
       "6                         4  \n",
       "7                         2  \n",
       "8                         2  \n",
       "9                        11  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that normally don't reachs the destination. We can see that the number of states explored increases as iterations are done. And we can see that normally do a lot of traffic violations.\n",
    "\n",
    "Let's see now the 10 last iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration</th>\n",
       "      <th>success</th>\n",
       "      <th>cum_reward</th>\n",
       "      <th>explored_states_cum</th>\n",
       "      <th>traffic_violations_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>91</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>92</td>\n",
       "      <td>False</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>93</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>94</td>\n",
       "      <td>False</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>95</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>False</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>71</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>False</td>\n",
       "      <td>-10.5</td>\n",
       "      <td>73</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>74</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>True</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>75</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>-6.5</td>\n",
       "      <td>77</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    iteration success  cum_reward  explored_states_cum  \\\n",
       "90         91   False        -0.5                   69   \n",
       "91         92   False        -3.5                   69   \n",
       "92         93   False         1.0                   69   \n",
       "93         94   False        -6.0                   71   \n",
       "94         95    True        -1.0                   71   \n",
       "95         96   False        -7.0                   71   \n",
       "96         97   False       -10.5                   73   \n",
       "97         98   False         3.0                   74   \n",
       "98         99    True        -3.0                   75   \n",
       "99        100   False        -6.5                   77   \n",
       "\n",
       "    traffic_violations_count  \n",
       "90                         2  \n",
       "91                         1  \n",
       "92                         0  \n",
       "93                         1  \n",
       "94                         2  \n",
       "95                         6  \n",
       "96                        10  \n",
       "97                        11  \n",
       "98                         9  \n",
       "99                        21  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We see more or less the same behaviour that in the first iterations, so we can conclude that *QLearningAgent* is \n",
    "not learning well.\n",
    "\n",
    "\n",
    "\n",
    "Let's see the number of times that the *QLearningAgent* has been successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "print len(data[(data.success)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *QLearningAgent* has a 38% of success.\n",
    "\n",
    "I've done 100 simulations with the *RandomAgent* and *enforce_deadline=True*, and it has a 16% of success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4. Improve the Q-Learning Driving Agent</h2>\n",
    "\n",
    "Now let's tune the values for the **learning rate (alpha)**, **the discount factor (gamma)** and the **exploration rate (epsilon)**. \n",
    "\n",
    "We will perform many simulations with many combinations of these parameters, and the we will report the results to see what \n",
    "is the best combination. The correspoding code used to generate this data can be found on **smartcab/main_qlearn_agent_tuning.py**.\n",
    "\n",
    "The results of the simulations will be stored on a csv file, that we will analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>alpha_rate</th>\n",
       "      <th>epsilon_rate</th>\n",
       "      <th>gamma_rate</th>\n",
       "      <th>successPerc</th>\n",
       "      <th>actionsAvg</th>\n",
       "      <th>cumRewardAvg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>499.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>21.585000</td>\n",
       "      <td>26.762000</td>\n",
       "      <td>-1.201770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>288.819436</td>\n",
       "      <td>0.319302</td>\n",
       "      <td>0.319302</td>\n",
       "      <td>0.319302</td>\n",
       "      <td>11.008268</td>\n",
       "      <td>1.983252</td>\n",
       "      <td>3.540282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>-15.305000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>249.750000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>-2.835000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>499.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>-2.142500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>749.250000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>-0.788750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>999.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>22.350000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0   alpha_rate  epsilon_rate   gamma_rate  successPerc  \\\n",
       "count  1000.000000  1000.000000   1000.000000  1000.000000  1000.000000   \n",
       "mean    499.500000     0.500000      0.500000     0.500000    21.585000   \n",
       "std     288.819436     0.319302      0.319302     0.319302    11.008268   \n",
       "min       0.000000     0.000000      0.000000     0.000000     0.000000   \n",
       "25%     249.750000     0.222222      0.222222     0.222222    16.000000   \n",
       "50%     499.500000     0.500000      0.500000     0.500000    20.000000   \n",
       "75%     749.250000     0.777778      0.777778     0.777778    24.000000   \n",
       "max     999.000000     1.000000      1.000000     1.000000    97.000000   \n",
       "\n",
       "        actionsAvg  cumRewardAvg  \n",
       "count  1000.000000   1000.000000  \n",
       "mean     26.762000     -1.201770  \n",
       "std       1.983252      3.540282  \n",
       "min      13.000000    -15.305000  \n",
       "25%      26.000000     -2.835000  \n",
       "50%      27.000000     -2.142500  \n",
       "75%      28.000000     -0.788750  \n",
       "max      31.000000     22.350000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tuning_data = pd.read_csv(\"smartcab/qlearn_agent_tuning_results.csv\")\n",
    "tuning_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine a suitable set of parameters, lets find: the q-agent with maximum success percentage, the q-agent with minimum actions taken average, and the q-agent with maximum accumulated reward average.\n",
    "\n",
    "<h4>Q-Agent with maximum success percentage</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      506.000000\n",
       "alpha_rate        0.555556\n",
       "epsilon_rate      0.000000\n",
       "gamma_rate        0.666667\n",
       "successPerc      97.000000\n",
       "actionsAvg       14.000000\n",
       "cumRewardAvg      9.820000\n",
       "Name: 506, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning_data.loc[tuning_data['successPerc'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This agent achieved a 97% success percentage. This is really a good percentage, so we can discard a bad implementation of the Q algorithm. It seems I selected bad values for the parameters in the previous section.\n",
    "\n",
    "Let's name this agent **SuccessAgent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Q-Agent minimum actions taken average</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      703.000000\n",
       "alpha_rate        0.777778\n",
       "epsilon_rate      0.000000\n",
       "gamma_rate        0.333333\n",
       "successPerc      95.000000\n",
       "actionsAvg       13.000000\n",
       "cumRewardAvg      8.870000\n",
       "Name: 703, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning_data.loc[tuning_data['actionsAvg'].idxmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This agent has an average of 13.000 actions taken.\n",
    "\n",
    "Let's name this agent **ActionsAgent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Q-Agent maximum accumulated reward average</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      103.000000\n",
       "alpha_rate        0.111111\n",
       "epsilon_rate      0.000000\n",
       "gamma_rate        0.333333\n",
       "successPerc      31.000000\n",
       "actionsAvg       25.000000\n",
       "cumRewardAvg     22.350000\n",
       "Name: 103, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning_data.loc[tuning_data['cumRewardAvg'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This agent has an average of 22.350 accumulated reward.\n",
    "\n",
    "Let's name this agent **RewardAgent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>And the winner is...</h4>\n",
    "\n",
    "The **RewardAgent** has a poor success percentage so let's discard it.\n",
    "\n",
    "The difference between **SuccessAgent** and **ActionsAgent** is minimal. But the **SuccessAgent** has a slightly better accumulated reward average. So I will choose **SuccessAgent** as the winner.\n",
    "\n",
    "The parameter values for **SuccessAgent** are: **alpha_rate = 0.555556**, **epsilon_rate = 0.000000**, **gamma rate = 0.666667**."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
